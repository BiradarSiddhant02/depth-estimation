{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from model import TransferLearning, TorchUNET, CustomUNET1, CustomUNET2\n",
    "import argparse\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_FOLDER = \"../models\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_PATH = \"../inputs/nyu_data/data/nyu2_test\"\n",
    "MODEL_CLASS = \"custom-UNET\"\n",
    "MODEL_GEN = \"gen-2\"\n",
    "\n",
    "MODEL_PATH = os.path.join(MODELS_FOLDER, MODEL_CLASS, MODEL_GEN)\n",
    "SAVE_PATH = os.path.join(\"../outputs\", MODEL_CLASS, MODEL_GEN)\n",
    "\n",
    "POOL_SIZE = 3\n",
    "STRIDE = (POOL_SIZE - 1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in os.listdir(MODEL_PATH):\n",
    "    tokens = model.split(\".\")\n",
    "    if tokens[0] == \"3\" or tokens[0] == \"7\":\n",
    "        model_path = model \n",
    "\n",
    "if MODEL_CLASS == \"torch-UNET\":\n",
    "    torchUNET = TorchUNET()\n",
    "    depth_model = torchUNET.get_model().to(DEVICE)\n",
    "    depth_model.load_state_dict(\n",
    "        torch.load(os.path.join(MODEL_PATH, model_path), map_location=DEVICE, weights_only=True)\n",
    "    )\n",
    "\n",
    "elif MODEL_CLASS == \"Transfer-Learning\":\n",
    "    depth_model = TransferLearning().to(DEVICE)\n",
    "    depth_model.load_state_dict(\n",
    "        torch.load(os.path.join(MODEL_PATH, model_path), map_location=DEVICE, weights_only=True)\n",
    "    )\n",
    "\n",
    "elif MODEL_CLASS == \"custom-UNET\" and MODEL_GEN == \"gen-1\":\n",
    "    depth_model = CustomUNET1().to(DEVICE)\n",
    "    depth_model.load_state_dict(\n",
    "        torch.load(os.path.join(MODEL_PATH, model_path), map_location=DEVICE, weights_only=True)\n",
    "    )\n",
    "\n",
    "elif MODEL_CLASS == \"custom-UNET\" and MODEL_GEN == \"gen-2\":\n",
    "    depth_model = CustomUNET2().to(DEVICE)\n",
    "    depth_model.load_state_dict(\n",
    "        torch.load(os.path.join(MODEL_PATH, model_path), map_location=DEVICE, weights_only=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 1308/1308 [00:08<00:00, 146.64it/s]\n"
     ]
    }
   ],
   "source": [
    "image_data = {}\n",
    "\n",
    "\"\"\"\n",
    "STORAGE FORMAT:\n",
    "{\n",
    "    key1 : [image1, depth1],\n",
    "    key2 : [image2, depth2]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Read images from the given path\n",
    "image_files = os.listdir(DATA_PATH)\n",
    "\n",
    "# Classify and store images based on their endings\n",
    "for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "    image_name, ext = os.path.splitext(image_file)\n",
    "    key = image_name.split(\"_\")[0]  # Extract the part before '_'\n",
    "\n",
    "    image_path = os.path.join(DATA_PATH, image_file)\n",
    "\n",
    "    if image_file.endswith(\"_depth.png\"):\n",
    "        if key in image_data:\n",
    "            image_data[key] = (image_data[key][0], cv2.imread(image_path))\n",
    "        else:\n",
    "            image_data[key] = (None, cv2.imread(image_path))\n",
    "\n",
    "    elif image_file.endswith(\"_colors.png\"):\n",
    "        if key in image_data:\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_data[key] = (image, image_data[key][1])\n",
    "        else:\n",
    "            image_data[key] = (cv2.imread(image_path), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (Inference Time: 0.0412 s/image): 100%|██████████| 654/654 [00:28<00:00, 23.16it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "progress_bar = tqdm(image_data.items(), desc=\"Evaluating\", total=len(image_data))\n",
    "\n",
    "for key, (color_img, ground_truth) in progress_bar:\n",
    "\n",
    "    image_tensor = (\n",
    "        torch.from_numpy(cv2.resize(color_img, (320, 240)))\n",
    "        .float()\n",
    "        .permute(2, 0, 1)\n",
    "        .unsqueeze(0)\n",
    "        .to(DEVICE)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start = time()\n",
    "        predicted = depth_model(image_tensor).cpu().squeeze().numpy()\n",
    "        end = time()\n",
    "    \n",
    "    inference_time = end - start\n",
    "\n",
    "    predicted_min = predicted.min()\n",
    "    predicted_max = predicted.max()\n",
    "    predicted = (predicted - predicted_min) / (predicted_max - predicted_min)\n",
    "\n",
    "    progress_bar.set_description(f\"Evaluating (Inference Time: {inference_time:.4f} s/image)\")\n",
    "\n",
    "    outputs.append((color_img, ground_truth, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving images: 100%|██████████| 654/654 [00:18<00:00, 35.94it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "for i, (color_img, ground_truth, predicted) in tqdm(\n",
    "    enumerate(outputs), desc=\"Saving images\", total=len(outputs)\n",
    "):\n",
    "    filename = f\"{i}.png\"\n",
    "    plt.imsave(os.path.join(SAVE_PATH, filename), predicted, cmap=\"viridis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
